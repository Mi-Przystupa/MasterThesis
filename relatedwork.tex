\chapter{Related Work}

As previously mentioned, \ac{LVNMT} has been an active area of research in recent years. The original discriminative model was proposed by \citet{Zhang2016VNMT} who found it to improved general translation and gave better translations on longer sequences. This model was then extended by the work of \citet{Su2018VRNMT} to sequential variational \ac{RNN}s providing again better longer sentence translations. An alternative formulation of the variational distribution was proposed by \citet{schulz2018StochasticDecoder} where the emphasis was on diversifying translations. In the generative modeling approach the idea was proposed by the works of \citet{harshil2018GNMT} which enabled word imputation translation as well as a form of unsupervised translation in the \ac{LVNMT} framework. Concurrently, \citet{eikema2018AEVNMT} found the generative formulation to be marginally better than the discriminative model without the latent variable and showed these models could be slightly more robust to domain mismatch. 

Latent variables have also been considered in alternative formulations. \citet{shen2019mixturemodelsfordiverseMT} apply mixture models for diverse translation. \citet{kaiser2018DiscreteLatentVariables} faster decoding with discrete latent variable. \citet{he2018seq2seqmixturemodelforDiverseMachineTranslation} introduce latent variable to improve translation. I'm pretty sure 2 of these are the same paper but...let's pretend they aren't. 

To our knowledge, the applications of normalizing flows have been considered sparsely in natural language processing, and largely focused on language modeling. \citet{bowman2015GeneratingSent} briefly mention normalizing flows in the context of variational auto-regressive language modeling, but did not report their empirical findings. \citet{ziegler2019LatentNFforDiscrete} provides empirical evidence on applying normalizing flows for character level language modeling. They proposed several non-autoregressive models that allow for parallel decoding, and provided evidence on the multi-modal behaviors of language factors. In this work, we focus on autoregressive approaches applied to neural machine translations.

Specific to research on developing normalizing flow functions, there has been recent work developing discrete normalizing flows. \cite{tran2019discreteflows,hoogeboom2019IntegerDiscreteFlows} propose similar flows that allow direct application of normalizing flows on discrete distributions. \reminder{read these papers and give the gist of them here}. \reminder{They do this by incorporating modulus mathematics and some something functions on top of this}.




\reminder{Misc things I'm not quite sure where to put yet...}
As previously mentioned, \ac{LVNMT} has been an active area of research in recent years. The original discriminative model was proposed by \citet{Zhang2016VNMT} who found it to improved general translation and gave better translations on longer sequences. This model was then extended by the work of \citet{Su2018VRNMT} to sequential variational \ac{RNN}s providing again better longer sentence translations. An alternative formulation of the variational distribution was proposed by \citet{schulz2018StochasticDecoder} where the emphasis was on diversifying translations. In the generative modeling approach the idea was proposed by the works of \citet{harshil2018GNMT} which enabled word imputation translation as well as a form of unsupervised translation in the \ac{LVNMT} framework. Concurrently, \citet{eikema2018AEVNMT} found the generative formulation to be marginally better than the discriminative model without the latent variable and showed these models could be slightly more robust to domain mismatch. 

To our knowledge, the applications of normalizing flows have been considered sparsely in natural language processing, and largely focused on language modeling. \citet{bowman2015GeneratingSent} briefly mention normalizing flows in the context of variational auto-regressive language modeling, but did not report their empirical findings. \citet{ziegler2019LatentNFforDiscrete} provides empirical evidence on applying normalizing flows for character level language modeling. They proposed several non-autoregressive models that allow for parallel decoding, and provided evidence on the multi-modal behaviors of language factors. In this work, we focus on autoregressive approaches applied to neural machine translations.

Specific to research on developing normalizing flow functions, there has been recent work developing discrete normalizing flows. \cite{tran2019discreteflows,hoogeboom2019IntegerDiscreteFlows} propose similar flows that allow direct application of normalizing flows on discrete distributions. \reminder{read these papers and give the gist of them here}. \reminder{They do this by incorporating modulus mathematics and some something functions on top of this}.


