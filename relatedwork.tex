\chapter{Related Work}

To our knowledge, the applications of normalizing flows have been considered sparsely in natural language processing, and largely focused on language modeling. \citet{bowman2015GeneratingSent} briefly mention normalizing flows in the context of variational auto-regressive language modeling, but did not report their empirical findings. \citet{ziegler2019LatentNFforDiscrete} provides empirical evidence on applying normalizing flows for character level language modeling. They proposed several non-autoregressive models that allow for parallel decoding, and provided evidence on the multi-modal behaviors of language factors. In this work, we focus on autoregressive approaches applied to neural machine translations.