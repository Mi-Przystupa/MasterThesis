\chapter{Normalizing Flows in Machine Translation}

%In this chapter we discuss the implementation details for the \ac{LVNMT} models we consider for analysis. Along with this includes several design choices we made based on related research and our own findings in the preliminary experiments section. 

In this chapter we discuss incorporating normalizing flows into \ac{LVNMT} systems. This includes descriptions of the normalizing flows we considered,and the challenges with incorporating normalizing flows, particularly with auto-regressive models like those we considered.

\section{Applying Flows to Latent Variables}


Incorporating normalizing flows into existing \ac{LVNMT} systems is relatively straight forward. During the training procedure, one only needs to apply $k$ functions $f_{i}$ sequentially to samples from the base distribution $p(z_{0})$: 

\begin{equation}
z_{k} = f_{k} \circ f_{k-1} ... \circ f_{2} \circ f_{1}(z_{0}) , z_{0} \sim p(z_{0})
\end{equation}

$p(z_{0})$ refers to our variational posterior distribution when training \ac{LVNMT} models with normalizing flows. When evaluating translation quality, we set the latent variable to the expected value of the Gaussian distribution , $z_{0} = \mu_{\theta}(x)$, and apply our flows to this value.

\reminder{ move talking about objective in this section as more general case}

%\subsection{Affine Transforms of z}

%One observation we had of several \ac{LVNMT} we considered for our analysis was the introduction of affine transformation on the samples $z$ before passing them to the associated networks \cite{eikema2018AEVNMT,Zhang2016VNMT}. Interestingly, these networks could be viewed as normalizing flows if the inverse jacobian had been incorporated into the loss function. For our analysis, we consider scenarios where these affine transforms are removed as well as with the transforms included. When the affine layers are included, they are applied to the samples after the flows have been applied. 
\section{Considered Flows for Analysis}

There are a variety of flows considered in the literature, but for analysis we consider two types of flows. The Jacobian is different for each flow, offering alternative influences on the training procedure for our \ac{LVNMT} architectures. 

\subsection{Planar Flows}

Planar flows were proposed in the work of \citet{rezende2015VIwithNF} and are functions of the following form
\begin{equation}
f_{i}(z) = z + u_{i} h(w_{i}^{T} z + b_{i})
\end{equation}
$u_{i}, w_{i} \in R^{d}$ and $b \in R$ are the parameters of planar flow $i$, and $h$ is a non-linear activation. For our experiments we use \textit{tanh} but the authors note that alternative activations are permissible. A convenient aspect of these flows is they provide an analytical term of the Jacobian
\begin{equation}
\det \bigg( \frac{\delta f}{\delta z} \bigg) = \bigg| 1 + u^{t} h'(w_{i}^{T} z + b_{i})w \bigg|
\end{equation} 
Intuitively, this transformation can be seen as contracting or expanding the samples $z$ along a single line in space. This is a simple transformation and requires many planar flows to represent complicated distributions. This need for more flows to effectively transform the distribution space makes them less computationally efficient compared to other flows, but offer a simple type of normalizing flow to evaluated against.

  %In the experiments of \citet{rezende2015VIwithNF} they required at least 32 flows before accurately recovering the posterior distributions.
%As these flows can be viewed as layers in a \ac{MLP} \cite{kingma2016IAF}, this aspect can make them less practical compared to other flows. 

%\reminder{not sure if it's worth mentioning next part..}

%Another drawback with planar flows is that their inverse is not guaranteed. \citet{rezende2015VIwithNF} propose a way to define the parameters such that the flows are invertible, but it still remains challenging to invert planar flows. This can be due to numerical instability in the activation function, which when inverted can lead to a non-existent value, or require numerical solvers to find a solution. Despite these potential pitfalls, they are a fairly simple transformation to compare the importance of more complex distributions for \ac{LVNMT}



%\subsection{Sylvester Flows}

%Sylvester flows refers to a class of normalizing flows which utilize \textit{Sylvester's determinant identity} to define the Jacobian \cite{vdberg2018sylvester}. In the general case, they take the following form
%\begin{equation}
%	f_{i}(z) = z + A_{i} h(B_{i}z + b_{i})
%\end{equation}
%where $A_{i} \in R^{D \times M}$, $B_{i} \in R^{M \times D}$, $b_{i} \in R^{m}$, and $h$ is a non-linear activation. These types of flows can be viewed as a generalization of planar flows which transform a sample along several dimensions instead of a single one. Likewise, Sylvester flows determinant take a similar analytic form
%\begin{equation} 
%	\det \bigg( \frac{\delta f}{\delta z} \bigg) = \bigg| I_{M} + \text{diag} ( h'(B_{i}^{T} z ) + b_{i})B_{i}A_{i} \bigg|
%\end{equation}
%As in the planar flow scenario, the previously shown equations are not generally invertible. To guarantee invertibility, the authors define the learn-able parameters, as upper triangular matrices $R, \tilde{R} \in R^{M \times M}$, and orthonormal matrix $Q \in R^{D \times M}$ choosing to define $A_{i} = QR$ and $B=\tilde{R}Q^{T}$. In this framework, the authors proposed several variants to define $Q$. For our experiments we consider the Householder Sylvester Flow which defines Q with the House holder transformation.
%\begin{equation}
%	Q(z) = z - 2 \frac{vv^{T}}{||v||^{2}} z
%\end{equation}
%For further details on other considered variants, we refer readers to \citet{vdberg2018sylvester}. 

%\subsection{House Holder Flows}

%House holder flows refers specifically applying the House Holder matrix as a normalizing flow \cite{tomczak2016Householder}. This flow again make use of the previously discussed house holder transformation where the parameters are the vector $v$. In the work of \citet{tomczak2016Householder} the authors apply the observation that decomposing a full covariance matrix into it's orthogonal eigenvalue and eigenvector components one can use the eigenvector components as a series of transformed Gaussian distributions. Through the \textit{Basis-Kernel Representation of othorgonal matrices} theorem, this enables the application of House holder matrices, which are orthogonal, as normalizing flows. 

%In these flows, the parameter vectors $v$ are each conditioned on the previous flow parameters, with the exception of $v_{0}$ which is the hidden layer from a neural network. In the context of VAEs this refers specifically to the penultimate layer of the variational network $q_{\phi}(z \cond{x})$


\subsection{Inverse Autoregressive Flows}

Inverse autoregessive flows allow for parallel sampling by defining the function as the inverse operation in a sequence of random variables \citet{kingma2016IAF}
\begin{equation}
\epsilon_{i} = \frac{z_{i} - \sigma(x)}{\mu(x)}
\end{equation}

where $z_{i}$ is the dimensions of the variable of interest. This definition provides a lower triangular Jacobian matrix which means the determinant is just a product of the diagonal terms
\begin{equation}
\bigg| \log \frac{\delta f}{\delta z_{t -1 } } \bigg| = \prod_{t=0}^{T} \sigma_{t}
\end{equation}

The inverse is then defined as sequentially dependent which make these flows computationally expensive for density estimation. 

\begin{equation}
y_{i} = \mu + \sigma * \epsilon_{<i}
\end{equation}

In the authors work they use autoregressive MADE \cite{gregor2015MADE} and generate a context $h$ which each flow conditions on in addition to other inputs. 


\subsection{Flows Ammortization}

\reminder{talk more about what is ammortization and what it looks like in your models. there's lots of ways to do this}

\reminder{ what does it mean}

\reminder{ We choose to condition flows on only source sentence X}

\reminder{ for discriminative model where we learn both prior and variational distribution, we only condition on X and apply flows to both models. the rational is that the flows will always be used in either case and would ideally be perfect matches of each other}

One consideration when applying normalizing flows is whether to amortize flow parameters or learn them directly. In the amortized setting, the flows parameters are generated by a \textit{hyper-network} \cite{ha2016hypernets} which is a neural network that outputs the parameters of the normalizing flow. These data condition parameters enable more flexible per datum distributions. Previous research suggest these data conditioned flows provide better performance compared to data agnostic flows in which the parameters are treated as directly learned parameters \cite{vdberg2018sylvester}. \reminder{I think they were talking more in HOW you ammortize the flows i.e. changing the input versus learning kinda the FULL layer's parameters}

For our experiments, we consider both settings to test whether previously reported performance gains carry over to the translation scenario. When amortizing the flow parameters, our hyper-networks are simply affine layers which condition on the same inputs as our variational distributions. As an example, for a single planar flow this will look like the following 
\begin{equation}
	affine(h_{xy}) = (u, w, b)
\end{equation}
where $u_{i}, w_{i} \in R^{d}$ and $b \in R$ are the parameters of flow which are produced by the affine layer instead of being directly learned parameters.





%A subtle point in flows literature is how the parameters of the flows are learned. In \ac{VAE} style set-up, it is possible to data conditioned flow parameters or treat the parameters as directly learnable. It has previously been cited that data conditioned flow parameters can produce better performance that data agnostic flows \cite{vdberg2018sylvester}. An advantage of an agnostic, or global, interpretation of flows is that this allows easier interpolation between latent samples between data points, which is one of the motivations for \ac{VAE} style models. 

%\reminder{talk about ammorization of flow parameters}



\section{Regulization with Latent Variables}

\reminder{just talk about 2 forms of regulization we use}

\reminder{blah blah, KL annealing used in basically all NF literature, also word drop has been effective (can even include pseudo code version if you want)}

The one other consideration with the inclusion of normalizing flows is how they change the \ac{ELBO}. Here we present a formulation of the \ac{ELBO} specific to machine translation which is based on the derivation from \citealp[Section 4.2]{rezende2015VIwithNF}.

\begin{align}
\begin{split}\label{eq:2}
&
E_{q(\textbf{z}_{0}\cond{\textbf{x}, \textbf{y}})} \bigg[ \sum_{j=1}^{p} \log p_{\theta}(y_{j} \cond{\textbf{z}^{(k)}}, \textbf{x}, y_{<j}) \bigg] \\
& - KL(q_{\phi}(\textbf{z}_{(0)} \cond{\textbf{x}, \textbf{y}}) || p_{\theta}(\textbf{z}^{(k)} \cond{\textbf{x}})) \\
&   +  E_{q_{\phi}(\textbf{z}_{0} \cond{\textbf{x}, \textbf{y}})} \bigg[\sum_{k=1}^{K} \log \bigg|\frac{\delta f^{(k)}}{\delta \textbf{z}^{(k-1)}} \bigg| \bigg]  
\end{split}
\end{align}

The first term represent maximizing the likelihood of observed sequences i.e. translating data correctly. The other two terms represent the introduced regularization from the latent variable $\textbf{z}$ in the model. 
%fyi
%https://tex.stackexchange.com/questions/44450/how-to-align-a-set-of-multiline-equations

The problem with this objective is that the inclusion of this KL divergence can lead to a problem referred to as ''posterior collapse'' \cite{he2018lagging}. This refers to the scenario where, in order to maximize the ELBO, the variational distribution parameters, for all the training data, are pushed to more closely match the prior distribution parameters. In the typical case where the prior is the unit Gaussian distribution, this leads to uninformative codes in which case the latent variable provides no additional information to the model. We recommend \citet{chen2016VariationalLossyAE} or \citet{zhao2017InfoVAE} which provide more thorough discussions on the subject. 

 

For this work, we address this potential problem with a previously proposed approach referred to as KL-annealing \cite{bowman2015GeneratingSent,sonderby2016LadderVAE}. KL-annealing is the process of annealing the weight associated to the divergence term in the ELBO from 0.0 (no influence) to 1.0 (original weight). We use a linear annealing schedule which increases the importance of our regularization terms after each mini-batch update. 

We consider several variations of KL-annealing in our analysis which have previously been previously in other works. This include annealing both terms of the KL equally  $\beta (E_{q(z_{k} \cond{x}))}[\log(q(z_{k} \cond{x})]$ , annealing our joint distribution $\beta p(x, z_{k})$ , and a variation we were in which we only anneal the entropy of the log likelihood of the prior. 
%\begin{align}
%\begin{split}
%&
%E_{q(\textbf{z}_{0}\cond{\textbf{x}, \textbf{y}})} \bigg[ \beta \log p_{\theta}(y, \textbf{z}^{(k)}, \textbf{x}) \bigg]  - E_{q_{\phi}(\textbf{z}_{0} \cond{\textbf{x}, \textbf{y}})} \bigg[q_{\phi}(\textbf{z}_{(0)} \cond{\textbf{x}, \textbf{y}}) - \sum_{k=1}^{K} \log \bigg|\frac{\delta f^{(k)}}{\delta \textbf{z}^{(k-1)}} \bigg| \bigg]  
%\end{split}
%\end{align}

%\begin{align}
%\begin{split}\label{eq:2}
%&
%E_{q(\textbf{z}_{0}\cond{\textbf{x}, \textbf{y}})} \bigg[ \sum_{j=1}^{p} \log p_{\theta}(y_{j} \cond{\textbf{z}^{(k)}}, \textbf{x}, y_{<j}) \bigg] \\
%& - \beta KL(q_{\phi}(\textbf{z}_{(0)} \cond{\textbf{x}, \textbf{y}}) || p_{\theta}(\textbf{z}^{(k)} \cond{\textbf{x}})) \\
%&   +  \beta E_{q_{\phi}(\textbf{z}_{0} \cond{\textbf{x}, \textbf{y}})} \bigg[\sum_{k=1}^{K} \log \bigg|\frac{\delta f^{(k)}}{\delta \textbf{z}^{(k-1)}} \bigg| \bigg]  
%\end{split}
%\end{align}

%\begin{align}
%\begin{split}\label{eq:2}
%&
%E_{q_{\phi}(\textbf{z}_{0}\cond{\textbf{x}, \textbf{y}})} \bigg[ \sum_{j=1}^{p} \log p_{\theta}(y_{j} \cond{\textbf{z}^{(k)}}, \textbf{x}, y_{<j}) \bigg] \\
%& - E_{q_{\phi}(\textbf{z}_{0}\cond{\textbf{x}, \textbf{y}})}  \bigg[ q_{\phi}(\textbf{z}_{(0)} \cond{\textbf{x}, \textbf{y}}) - \sum_{k=1}^{K} \log \bigg|\frac{\delta f^{(k)}}{\delta \textbf{z}^{(k-1)}} \bigg| \bigg] \\
%&   +  \beta E_{q_{\phi}(\textbf{z}_{0} \cond{\textbf{x}, \textbf{y}})} \bigg[log p_{\theta}(\textbf{z}^{(k)} \cond{\textbf{x}}) \bigg]  
%\end{split}
%\end{align}

