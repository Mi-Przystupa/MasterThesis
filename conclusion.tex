\chapter{Conclusion}

In this work we considered the inclusion of normalizing flows in existing \ac{LVNMT} models. Our results would suggest that \ac{VNMT} benefited more from the inclusion of normalizing flows than \ac{GNMT}. We accredit this to the input-feeding approach of $z$ in \ac{VNMT} as opposed to the initialization approach of \ac{GNMT}. This is because, even without the language model, we find that neither $z$ or the optimization of the language model seem to account for our \ac{GNMT} performance gains. Interestingly, from our probing of the latent variable $z$, it seems performance gains may be accredited in part due to the stochastic behaviour introduced by $z$ instead of it's exact value. 

We believe normalizing flows do have much future potential benefits in machine translation systems. However, they are not necessarily something that can be added for a guaranteed performance benefit. To properly see benefit from normalizing flows, additional considerations must be taken into account. One potential future work is extending our findings to sequential latent variable \ac{NMT} systems. These have previously been introduced to help with longer sentence translation as well as diversifying translation. Another direction could be considering joint modelling of flow based models to generate synthetic sentence paired data. Synthetically generated data has been studied in machine translation as one approach to improve existing systems performances \cite{sennrich2015ImprovingNMT}. Most recently, discrete flows have been proposed showing improvement on existing flow based language modelling results \cite{tran2019discreteflows}. Their the authors cite the vocabulary size as a limiting factor, but from our results with a small \ac{BPE} vocabulary it may be plausible to still utilize these flows effectively for translation. 



 