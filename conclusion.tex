\chapter{Conclusion}

In this work we considered the inclusion of normalizing flows in existing \ac{LVNMT} models. Whether with attention or without attention, our \ac{VNMT} model seemed to benefit from the inclusion of normalizing flows. Particularly when attention was removed, our probing of the latent $z$ found that models would depend more so on the latent variable $z$ for translation quality. In contrast, \ac{GNMT} did not benefit as much in any setting we included normalizing flows. Although we saw more dependence on the latent $z$ when removing attention for \ac{GNMT}, our baseline models often had better final translation performance. One explanation from this came to light from removing the optimization of the language model, in which case our \ac{GNMT} model completely ignored the latent $z$ for translation. This might suggest the initialization approach to including $z$ in our \ac{GNMT} implementation is less effective than the input-feeding approach of \ac{VNMT} for enabling normalizing flows to improve translation performance.

We caution interpretations of our results given the breadth of hyper-parameters to consider, such as the amount of word-dropout, KL-annealing, or even \ac{MC} samples used for approximating the \ac{ELBO}. Given these other factors, it can be difficult to fully give credit to normalizing flows themselves for performance gains. Understanding how different annealing schedules might impact normalizing flows is a promising future research direction itself, particularly given understanding KL annealing is an active area of research \cite{he2018lagging,sphericallatent2018Xu}.


%Our results would suggest that \ac{VNMT} benefit more from the inclusion of normalizing flows than \ac{GNMT}. We accredit this to the input-feeding approach of $z$ in \ac{VNMT} as opposed to the initialization approach of \ac{GNMT}. This is because, even without the language model, we find that neither $z$ or the optimization of the language model seem to account for our \ac{GNMT} performance gains. Interestingly, from our probing of the latent variable $z$, it seems performance gains may be accredited in part to the stochastic behaviour introduced by $z$ instead of its exact value. 


Despite these considerations, we believe normalizing flows do have much future potential benefits in machine translation systems, particularly given previous success in non-autoregressive translation \cite{flowseq2019Xuezhe}. However, normalizing flows are not necessarily something that can be added for a guaranteed performance benefit. To properly see benefit from normalizing flows, additional considerations must be taken into account. One potential future work is extending our findings to sequential latent variable \ac{NMT} systems. These have previously been introduced to help with longer sentence translation as well as diversifying translation. Another direction could be considering joint modelling of flow based models to generate synthetic sentence paired data. Synthetically generated data has been studied in machine translation as one approach to improve existing systems performances \cite{sennrich2015ImprovingNMT}. Most recently, discrete flows have been proposed showing improvement on existing flow based language modelling results \cite{tran2019discreteflows}. Their the authors cite the vocabulary size as a limiting factor, but from our results with a small \ac{BPE} vocabulary it may be plausible to still utilize these flows effectively for translation. 



 