\chapter{Latent Variable Neural Machine Translation}

In this chapter we describe the \ac{LVNMT} models we considered as part of our analysis. We begin by describing the underlying \ac{NMT} architecture both of these approaches build off of. We then described a previously proposed discriminative model which models the conditional distribution $p(y \cond{x})$. We follow with discussion on our generative model which models the joint distribution $p(x, y)$. The chapter finishes with shared details applicable for both considered models. Throughout this chapter we will often reference vectors as $R^{n}$ but the dimensions for these vectors are not necessarily the same dimensions. 


\section{Neural Architecture}

In this section we explain the base neural architecture which our considered \ac{LVNMT} models build from. We chose this architecture in order to limit the number of manipulated parameters in our analysis, but this does not mean other architectures are incompatible in \ac{LVNMT} systems. The ideas in each \ac{LVNMT} model considered are applicable to alternative neural architectures, such as the Transformer \cite{vaswani2017attentionTransformer}, which may benefit from introduced latent variables as well. 

The \ac{NMT} architecture we consider is a \ac{seq2seq} model proposed by the work of  \citet{bahdanau2014NMTBYJoint}. The core components include source \& target word embeddings, an encoder, attention mechanism, and decoder. We describe all the layers except the word embeddings which are projections of the vocabulary for each language into a continuous space $R^{n}$. When we refer  words $x_{i}$ or $y_{i}$ these actually correspond to each word's associated word embedding as inputs to the model.  

\subsection{Encoder}
The encoder is a bi-directional \ac{RNN} which generates hidden states from reading the source sequence $x$ both forwards and backwards. Formally, the \ac{RNN} produces forward hidden states $h^{f}_{i} \in R^{n}$ for each input word $x_{i}$, where $h^{f}_{i}$ is conditioned on all previous $x_{<i}$ words. The sequence is then read backwards by the \ac{RNN} to produce hidden states $h^{b}_{i} \in R^{n}$ for each word $x_{i}$ where each $h^{b}_{i}$ is conditioned on all subsequent words in the sentence $x_{>i}$. The final hidden states are a concatenation of these complementary embeddings $h_{i}  = [h^{f}_{i} ; h^{b}_{i}]$ $ \forall i \in [0,...,T]$ where $T$ is the sentence length. Intuitively, each $h_{i}$ can be viewed as a contextual embedding of each source word in the sentence. This information improves the translation quality beyond simply initializing the decoder through the attention mechanism, described next. 

\subsection{Global Attention}

Global attention mechanisms, in the context of \ac{seq2seq} models, generally refer to combining information from all the encoder hidden states to inform the decoding process. The idea is to introduce some function on the current decoder state $s_{j}$ and encoder states $H \in R^{T \times n}$ to output an energy vector $e \in R^{T}$. In the work of \citet{bahdanau2014NMTBYJoint} the authors propose a \ac{MLP} attention function
\begin{equation}
e_{i} = MLP(h_{i}, s_{j}), \forall i \in [1...T]
\end{equation}


These energy values are usually normalized to provide weights $\alpha_{i}$ per hidden state. \citet{bahdanau2014NMTBYJoint} choose the softmax function for this operation 

\begin{equation}
	\alpha_{i} = \frac{exp(e_{i})}{\sum_{i=1}^{T} exp(e'_{i})}
\end{equation}
These then produce context vector $c_{j]}$ as follows. 
\begin{equation}
c_{j} = \sum_{1}^{T} \alpha_{i} * h_{i}
\end{equation}
The intuition for $c_{j}$ is that it captures alignment information between the source sequence and the current position in the translation sequence. % It should be noted that previous work as shown the attention alignment becomes less informative for longer sequence \reminder{cite this}. 

 




\subsection{Decoder}


The decoder is a feed-forward \ac{RNN} which encodes information to generate the information for translation. It reads the sequence forward producing hidden states $s_{j} \in R^{n}$ for each target word $y_{j}$ in the sequence of length $K$. In the literature, it can be viewed as a conditional language model \cite{koehn2017NMT} whose hidden state is initialize as $s_{0} = tanh(affine(h_{0})$. The decoder has three inputs which include the previous word $y_{j-1}$, the previous hidden state $s_{j-1}$ and the context vector $c_{j}$ as mentioned in our discussion on the global attention mechanism. 


To generate the probabilities for each word in the target sentence, the decoder includes a \ac{MLP} which uses the max out activation over the hidden state values. 
\begin{equation}
	t_{j} = maxout(affine(s_{j}))
\end{equation}
\begin{equation}
	maxout(t) = [max(t_{2j -1}, t_{2j})]_{i=1}^{l / 2}
\end{equation}
The final affine layer uses these max out values as input to produce the probability distribution over the vocabulary as follows.
\begin{equation}
p(y_{j} \cond{x, y_{<j}}) = gen(t_{j})
\end{equation}
It is responsible for generating each word $y_{j}$ in the sequence from information produced by the decoder. It can be viewed as the distribution parameters of the conditional the previous words $y_{<j}$ and whole sentence $x$.


\section{Discriminative Translation Model}


The discriminative \ac{LVNMT} model we considered was proposed in the work of \citet{Zhang2016VNMT}. It was the first model to propose introducing latent variables in \ac{NMT} with a \ac{VAE} style approach. It models the conditional distribution ($p(y \cond{x})$ which is the typical modeling assumption in \ac{NMT}. In their \ac{LVNMT} model they introduce the latent variable $z$ in the following model. 

\begin{equation}
p(y \cond{x})  = \int p(y \cond{x,z}) p(z \cond{x}) dz
\end{equation}
In the above equation, as previously discussed, the integral over latent variable $z$ is considered intractable because of the inclusion of neural networks to approximate the distribution. In usual circumstances, this requires introducing a variational approximation $q(z \cond{\cdot})$ to maximize the \ac{ELBO}, but the authors instead optimize the following objectives. %We note that there are works that consider instead variables per time-step as well \cite{schulz2018StochasticDecoder,Su2018VRNMT}, although we do not consider them for our analysis.


\begin{equation}
E_{p_{\theta}(z \cond{x})}[log p_{\theta}(y \cond{x,z})] +  KL (q_{\phi}(z \cond{x, y} || p_{\theta}(z \cond{x})))
\end{equation}

The key discrepancy is that the translation term is approximated by sampling from the prior distribution. This is not an unreasonable choice as the discriminative model can be rewritten as an expectation with respect to the distribution $p(z \cond{x})$.

\begin{equation}
p(Y \cond{X})  = \int p(y \cond{x,z}) p(z \cond{x}) dz = E_{p(z \cond{x})}[p(y \cond{x,z})]
\end{equation}



This allows one to directly derive a lower bound of the log-likelihood with Jensen's inequality for the discriminative model. 

\begin{equation}
log P(Y \cond{X}) \geq E_{p(z \cond{x})}[log p(y \cond{x,z})]
\end{equation}

This allows one to frame the objective of \citet{Zhang2016VNMT} as maximizing a lower bound of an expectation with the inclusion of a regularizer $KL (q_{\phi}(z \cond{x, y} || p_{\theta}(z \cond{x})))$. Intuitively the idea for the KL in this context is for our prior to encode information from both languages.  \footnote{When we looked at their code we verified that this was how they actually did it} 
%Particularly as both distributions are parametrized, the distributions are quite likely to match each other in order to minimize the divergence term.

For our experiments, we instead model the more familiar ELBO in which the samples are generated by the variational distribution instead of the prior distribution, which is still parametrized. 

\begin{equation}
E_{q_{\phi}(z \cond{x, y})}  log[p(y \cond{x, z} ) ] - KL (q_{\phi}(z \cond{x, y} || p_{\theta}(z \cond{x})))
\end{equation}

In either formulation, our objectives deviate from the typical VAE models, as the prior is a parametrized distribution $p_{\theta}(z \cond{x})$. Often, the prior is chosen to be a stationary distribution such as $N(0, diag(\sigma^{2})$ in the diagonal Gaussian case. This raises a potential issue with this model for some particular use cases with \ac{VAE} style models. 

As both $q_{\phi}$ and $p_{\theta}$ are chosen from the same distribution classes, it is possible to achieve the minimal divergence $KL(q || p) = 0$. This could potentially cause degeneracy in the latent space, because the distributions are not anchored to a fixed target. This means the model can push each datum's latent distributions apart in order to maximize the translation objective term. This could mean our distributions shrink the variance which would enable more accurate translations per sentence essentially making the latent variable meaningless. In other contexts, a fixed distribution can encourage phenomena in the optimization process such as latent disentanglement, or interpolation between latent codes these would be unacceptable artifacts of our modelling choices. 

Despite these drawbacks, one notable utility of a parametrized prior is during the translation process itself. This comes directly because of our procedure to optimize the latent variable to encode the information in both sentences. This allows us to easily replace the variational distribution with the prior distribution. This is a convenient artifact of the training proceedure, which the generative model we consider propose a variation of this idea \cite{eikema2018AEVNMT}.  % This representation provides an interesting consideration as well when training \ac{VAE} models with autoregressive models known as "posterior collapse" which we discuss in chapter 4 when detailing considerations for incorporating normalizing flows. 

As the  posterior distribution conditions on both source and target sentences, we encode the target sentence $y$ as well during training. For the posterior, the input is then the concatenation of $h_{mean}^y$ and $h_{mean}^X$ . The outputs of the affine layer are passed to respective distributions layers $\mu(affine(h_{mean}))$ and $\sigma(\text{softplus} (affine(h_{mean})))$ which represent the sufficient statistics of a Gaussian distribution. The samples from these distributions are then included as additional inputs to the decoder at each timestep in the decoding process
\begin{equation}
y_{j} = decoder(s_{j-1}, y_{j-1}, c_{j-1}, z))
\end{equation}



\section{Generative Translation Model}

In our analysis of generative modeling for translation, we consider the model proposed in the works of \citet{eikema2018AEVNMT} and \citet{harshil2018GNMT}. In their work, the latent variable is included in the joint distribution which is marginalized out during translation.

\begin{equation}
	p(x, y) = \int p(Y \cond{X, z})  p(X \cond{z}) p(z) dz
\end{equation}

In this framework, the latent variable $z$ represents shared aspects of language between the source and target language. We again introduce a variational distribution $q( z\cond{x, y})$ to arrive at an ELBO similar to the one seen in our discriminative model. We expand the \ac{ELBO} of this objective to make it explicit the models we optimize.

\begin{equation}
E_{q_{\phi}(z \cond{x,y})} [ log p_{\theta}(y \cond{x, z})] +
E_{q_{\phi}(z \cond{x,y})} [ log p_{\theta}(x \cond{z})] -
	KL(q_{\phi}(z \cond{x,y}) || p(z))
\end{equation}

In this approach the latent variable $z$ is viewed as shared information between the language pair. Our primary goal is learning a translation system through the distribution $p(y \cond{X, z})$, but  as an artifact train a \ac{NLM} on the distribution $p(x \cond{z})$. We now discuss the details relevant to each individual distribution. 

\subsection{Neural Language Model}

This include a language model for the the distribution $P(X \cond{z})$. Here the language model works exactly the same as the decoder in our base translation system. The difference in this setting is initializing the hidden state of the \ac{RNN} is with the latent variable $z$. This varies from our discriminative model which concatenated the latent variable with each input, and initialized the decoder hidden state with one from the encoder. An alternative approach could be to follow the discriminative model approach as well, but  previous research has found no notable differences on performance when incorporating latent variables for decoding \cite{bowman2015GeneratingSent}. 

The latent variable $z$ is included in the same way for both the translation and language model. This is to say in either case, $z$ initialized the hidden state of the corresponding \ac{RNN}. In the translation system this means initializing the hidden state of the encoder with the latent variable $z$ to provide a global semantic to maintain during translation. In the language model this corresponds to initializing an \ac{RNN} that follows the \ac{NMT} architecture with the latent variable $z$ instead of the encoder's hidden state. 


\subsection{Shared Latent space} 

One difficulty with this model is that the prior $p(z)$ is not parametrized as in the discriminative model previously discussed. This means that the previous solution of replacing the variational distribution with the prior is not-applicable. This comes with certain trade offs depending on the problem at hand.
As the prior is fixed, our model is restricted to expand the latent space and so interpolating between sentence pairs or sampling directly from $p(z)$ is again possible. This could potentially enable synthetic sentence generation which is otherwise not possible in our discriminative model. As synthetic sentences have been shown to improve translation, it may be a feasible application for generative \ac{LVNMT} systems. 

The limitation of this approach, and the focus of this work, is that in order to produce a translation, we would require our target sentence $y$ during translation. This is because our variational posterior conditions on both source $x$ and $y$ to encoder latent factors between both sentences. To address this issue, \citet{eikema2018AEVNMT} proposed heuristic approaches to mitigate this issue ranging from simply ignoring $y$ at decoding to more complex such as introducing a surrogate variational distributions which are trained to produce parameters that are the same as the $q(z \cond{x, y})$. This latter suggestion can be achieved by by adding an additional divergence term between the two distribution. 

\begin{equation}
\text{ELBO} + KL( q_{\lambda} (z \cond{x} || q_{\phi}(z \cond{x, y})))
\end{equation}

The motivation is similar as in the discriminative model scenario where we want our decoding variational distribution to capture the information shared between both languages. As both distributions are parametrized and from the same variational family this additional KL term can more likely achieve the minimal amount of divergence where both distributions match each other. 

\reminder{Another idea: share the variational encoder parameters, and during training mask out either the source or target sentence. This could also be a sort of extension to bidirectional training where the language model can be treated as an biproduct translation system. }

%In this section, we attempt to more generally classify latent variable neural machine translation. The two main considerations we have seen in the literature are the type of distribution to model, and the number of latent variables. 

%We note that latent variables have also been considered in formulations that may not lend themselves to the framework as discussed here. An example of this is the work of \reminder{cite that paper that uses a latent variable to pick a decoder} where the latent variable is used to pick a decoder. 



\section{Representation of Latent Variables}

Agnostic to either models we consider, an important consideration is encoding the sentences $x$ and $y$ in order to encode information for the introduced distributions. In order to represent both the prior $p(z \cond{x})$ and variational distribution $q(z \cond{x, y})$, we need to introduce additional layers into our \ac{NMT} architecture. To represent each distribution, we include affine layers that amortizes the generation of the distribution parameters for each translation pair. For each distribution, the inputs to this affine layer is the output of a mean pool operation over the length of the sequence

\begin{equation}
h_{mean} = \frac{1}{T} \sum_{i=1}^{T} h_{i}
\end{equation}

These $h$ vectors then are passed to layers that amortize the distribution parameters for our latent variables. This includes $\mu(h)$ and $\sigma(h)$. Each of these functions introduce an additional affine layer to produce the parameters. The $\sigma$ function then also includes a softplus non-linearity in order to preserve the positive definite behavior of the distribution variance. These outputs represent the parameters for an isotropic Gaussian which is our chosen base distribution. 




