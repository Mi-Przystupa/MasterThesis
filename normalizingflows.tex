\chapter{Normalizing Flows in Machine Translation}

%In this chapter we discuss the implementation details for the \ac{LVNMT} models we consider for analysis. Along with this includes several design choices we made based on related research and our own findings in the preliminary experiments section. 

In this chapter we discuss our approach to incorporating normalizing flows into the \ac{LVNMT} systems discussed in the previous chapter. This includes descriptions of the normalizing flows we considered,and regularization techniques to improve the utilization of the latent variable with auto-regressive models like those we consider.

\section{Applying Flows to Latent Variables}


As a reminder, normalizing flows transform samples from a base distribution $p(z_{0}))$ to a samples of more complex distribution by applying $k$ invertible functions $f_{i}$ sequentially: 

\begin{equation}
z_{k} = f_{k} \circ f_{k-1} ... \circ f_{2} \circ f_{1}(z_{0}) , z_{0} \sim p(z_{0})
\end{equation}

In our \ac{LVNMT} systems, $p(z_{0})$ refers to our variational posterior distribution when training \ac{LVNMT} models with normalizing flows. In our \ac{LVNMT} systems, each $f_{i}$ can be viewed as additional network layer between the base Gaussian distribution and the designated part of the translation system that the latent variable Z is included (see figure).

We follow previous research which make flows data dependent \cite{rezende2015VIwithNF,Berg2018SylvesterNF,kingma2016IAF,tomczak2016Householder}. Generally speaking, this means each sentence pair can have unique transforms $f_{i}$  enabling more flexible latent distributions per sentence pair.  Agnostic to the flows particular data conditioning approach, which we describe for each flow in the next section, we condition our flows on the hidden representation $h_{mean}^{X}$ which is the average word embedding from the encoder of the source sentence which simplifies the decoding scheme.

During training, we then optimize the \ac{ELBO}, which we present again which is formulated more specifically to machine translation and is based on the derivation from \citealp[Section 4.2]{rezende2015VIwithNF}.

\begin{align}
\begin{split}\label{eq:2}
&
E_{q(\textbf{z}_{0}\cond{\textbf{x}, \textbf{y}})} \bigg[ \sum_{j=1}^{p} \log p_{\theta}(y_{j} \cond{\textbf{z}^{(k)}}, \textbf{x}, y_{<j}) \bigg] \\
& - KL(q_{\phi}(\textbf{z}_{(0)} \cond{\textbf{x}, \textbf{y}}) || p(\textbf{z}^{(k)} \cond{\textbf{x}})) \\
&   +  E_{q_{\phi}(\textbf{z}_{0} \cond{\textbf{x}, \textbf{y}})} \bigg[\sum_{k=1}^{K} \log \bigg|\frac{\delta f^{(k)}}{\delta \textbf{z}^{(k-1)}} \bigg| \bigg]  
\end{split}
\end{align}

Note that the biggest change is the inclusion of an entropy over the log determinant, and in the joint distribution case, there would be the inclusion of optimizing the language model portion. Unfortunately, in normalizing flows we cannot analytically derive the KL divergence and instead we sample several time to optimize the objective. When evaluating translation quality, we set the latent variable to the expected value of the Gaussian distribution , $z_{0} = \mu_{\theta}(x)$, and apply our flows to this value.

Specific to our discriminative \ac{LVNMT} system, we choose to share the flow parameters on both the prior distribution and variational distribution. This is a choice specific to this model because of the modelling assumption. At decode time, we replace the variational distribution with the prior distribution which as been trained to match the variational distribution. Learning separate flows for each distribution would otherwise unnecessary computation overhead as the two base distributions should ideally match each other. This also changes the above equation to cancel the Entropy of the contributions of the normalizing flows. 


%\reminder{ move talking about objective in this section as more general case}





%\reminder{talk more about what is ammortization and what it looks like in your models. there's lots of ways to do this}

%\reminder{ what does it mean}

%\reminder{ We choose to condition flows on only source sentence X}

%\reminder{ for discriminative model where we learn both prior and variational distribution, we only condition on X and apply flows to both models. the rational is that the flows will always be used in either case and would ideally be perfect matches of each other}


\section{Considered Flows for Analysis}

For our analysis we consider two types of flows from the literature, but note that there are a variety of choices as research, at the time of writing, is quite active. The Jacobian is different for each flow we consider, offering alternative influences on the training procedure for our \ac{LVNMT} architectures. 

\subsection{Planar Flows}

Planar flows were proposed in the work of \citet{rezende2015VIwithNF} and are functions of the following form
\begin{equation}
f_{i}(z) = z + u_{i} h(w_{i}^{T} z + b_{i})
\end{equation}
$u_{i}, w_{i} \in R^{d}$ and $b \in R$ are the parameters of planar flow $i$, and $h$ is a non-linear activation. For our experiments we use \textit{tanh} but the authors note that alternative activations are permissible. A convenient aspect of these flows is they provide an analytical term of the Jacobian
\begin{equation}
\bigg| \det \bigg( \frac{\delta f}{\delta z} \bigg)\bigg|  = \bigg| 1 + u^{t} h'(w_{i}^{T} z + b_{i})w \bigg|
\end{equation} 
Intuitively, this transformation can be seen as contracting or expanding the samples $z$ along a single line in space. This is a simple transformation and requires many planar flows to represent complicated distributions.  %This need for more flows to effectively transform the distribution space makes them less computationally efficient compared to other flows, but offer a simple type of normalizing flow to evaluated against.

Another drawback with planar flows is that their inverse is not guaranteed. \citet{rezende2015VIwithNF} propose a way to define the parameters such that the flows are invertible, but it still remains challenging to invert planar flows. This can be due to numerical instability in the activation function, which when inverted can lead to a non-existent value, or require numerical solvers to find a solution. Despite these potential pitfalls, they are a fairly simple transformation to compare the importance of more complex distributions for LVNMT

%Previous research suggest these data conditioned flows provide better performance compared to data agnostic flows in which the parameters are treated as directly learned parameters \cite{vdberg2018sylvester}.

As previously mentioned, the parameters of flows are generally made to be data dependent. In the case of planar flows this is achieved by utilizing a \textit{hyper-network} \cite{ha2016hypernets} which outputs the parameters of the flow:
 \begin{equation}
 MLP(h_{xy}) = (u, w, b)
 \end{equation}
 where $u_{i}, w_{i} \in R^{d}$ and $b \in R$ are the parameters of flow which are produced by a \ac{MLP} layer instead of being directly learned parameters. In our experiments each of our flows has a separate hyper-network with a single hidden layer with tanh activations to allow flows to have sufficient flexibility to transform distributions.
 
\subsection{Inverse Autoregressive Flows}

Inverse autoregessive flows were proposed by \citet{kingma2016IAF} to enable parallel sampling by defining an invertible function as a sequentially dependent  inverse scale and shift  operation in a sequence of random variables
\begin{equation}
\epsilon_{i} = \frac{z_{i} - \sigma(x)}{\mu(x)}
\end{equation}

where $z_{i}$ is the ith dimension of the vector of latent variable $z$. This definition provides a lower triangular Jacobian which means the log absolute Jacobian is a summation of the diagonal terms 
\begin{equation}
\bigg| \log \frac{\delta f}{\delta z_{t -1 } } \bigg| = \sum_{t=0}^{T} \sigma_{t}
\end{equation}

The inverse is then defined as sequentially dependent which make these flows computationally expensive for density estimation. 

\begin{equation}
y_{i} = \mu + \sigma * \epsilon_{<i}
\end{equation}

In the authors work they use autoregressive MADE \cite{gregor2015MADE} and generate a context $h$ which each flow conditions on in addition to other inputs. This additional context vector is how each flow is conditioned on the data, as otherwise the MADE layers parameters are shared between each datum. In our models, the context vector is the $h_{mean}^{x}$.







\section{Regularization tricks}



%fyi
%https://tex.stackexchange.com/questions/44450/how-to-align-a-set-of-multiline-equations

A often cited challenge including latent variables in autoregressive models is  \textit{posterior collapse} \cite{he2018lagging}. This refers to the scenario where, in order to maximize the ELBO, the variational distribution parameters, for all the training data, are pushed to more closely match the prior distribution parameters. In the typical case where the prior is the unit Gaussian distribution, this leads to uninformative codes in which case the latent variable provides no additional information to the model. Part of this behavior has been accredit to strong decoders, like autoregressive models which are flexible enough to model the output by ignoreing $z$. We recommend \citet{chen2016VariationalLossyAE} or \citet{zhao2017InfoVAE} which provide more thorough discussions on the subject. 
 

For this work, we address this potential problem with previously proposed approach referred to as KL-annealing \cite{bowman2015GeneratingSent,sonderby2016LadderVAE} and word dropout \cite{bowman2015GeneratingSent}. KL-annealing is the process of annealing the weight associated to the divergence term in the ELBO from 0.0 (no influence) to 1.0 (original weight). We use a linear annealing schedule which increases the importance of our regularization terms after each mini-batch update. 

\begin{align}
\begin{split}\label{eq:2}
&
E_{q(\textbf{z}_{0}\cond{\textbf{x}, \textbf{y}})} \bigg[ \sum_{j=1}^{p} \log p_{\theta}(y_{j} \cond{\textbf{z}^{(k)}}, \textbf{x}, y_{<j}) \bigg] \\
& - \beta KL(q_{\phi}(\textbf{z}_{(0)} \cond{\textbf{x}, \textbf{y}}) || p_{\theta}(\textbf{z}^{(k)} \cond{\textbf{x}})) \\
&   +  \beta E_{q_{\phi}(\textbf{z}_{0} \cond{\textbf{x}, \textbf{y}})} \bigg[\sum_{k=1}^{K} \log \bigg|\frac{\delta f^{(k)}}{\delta \textbf{z}^{(k-1)}} \bigg| \bigg]  
\end{split}
\end{align}

KL-annealing has also been used in much previous research with normalizing flows to improve performance even without strong decoders \cite{rezende2015VIwithNF,kingma2016IAF,tomczak2016Householder,Berg2018SylvesterNF,ziegler2019LatentNFforDiscrete}

Word dropout is a procedure during training time where with some probability $\rho$ the current word $x_{i}$ is replaced with the word for unknown. The intuition for it's effectiveness is it encourages the model to depend more on the latent variable for information at decoding. This approach has particularly been important for improving performance of generative version of \ac{LVNMT} \cite{harshil2018GNMT,eikema2018AEVNMT}.

