\chapter{Background}

In this section, we provide background information on several subjects related to the work in this thesis. This include discussion the formulation of the variational auto-encoders, normalizing flows, and a brief description of \ac{NMT} where we give further details in the specific NMT model considered in the chapter on \ac{LVNMT} models.  

\subsection{Recurrent Neural Networks}

\reminder{talk about the core idea of RNNs}


\subsection{Neural Machine Translation}

Neural machine translation refers to neural network models trained specifically for the trask of machine translation. The best of these models are currently the \textit{de-facto} approach in machine translation research providing \ac{SOTA} performance on a number of benchmark language pair data sets.
 A variety of different architectures have been considered in the literature. These include CNN architectures which apply convolutions to generate sequences \reminder{cite sources}, \ac{RNN} architectures \cite{bahdanau2014NMTBYJoint,cho2014GRU}, the Transformer \reminder{cite vaswani}, and many other variations. There are many options for translation systems, but at the core of the most successful models are 

The most prominent architectures generally are \ac{seq2seq} models ~\cite{koehn2017NMT} have emerged as the most prominent architecture in the NMT literature. In seq2seq models, source sentences $X$ are \textit{encoded} as a series of latent representations capturing words in context information. A \textit{decoder} utilizes these hidden states, such as for initialization, to help inform the decoding process for target sentences $Y$. For our work, we consider both a recurrent neural network (RNN) with \textit{attention} and \textit{Transformer} seq2seq models for our experiments. We briefly introduce each of these next.


\section{Variational Autoencoders}

Variational autoencoders are a class of generative models that model the joint distribution $P(X, z)$, where $X$ is the dataset and $z$ is an introduced random variable. The joint distribution is factorized typically in the following form $P(X, z) = P(X\cond{z})P(z)$. This can be interpreted as assuming the dataset $X$ was generated by some latent process represented as a random variable $z$. To calculate the marginal probability $P(X)$, one then has to integrate out the random variable $z)$.
\begin{equation}
	P(X) = \int P(X \cond{z}) P(z) dz
\end{equation}

Unfortunately, this integral is generally considered to be intractable and a variational distribution $q(z \cond{X})$ is introduced to approximate the true $P(X)$. This is accomplished by minimizing the \ac{ELBO}, or variational free energy, the joint distribution $P(X, z)$ and $q(z \cond{X})$. 

\begin{equation}
	\E_{q(z \cond{X})} [ log(P(X, z) - log q(z \cond{X})] 
\end{equation}


In order to optimize this objective requires Monte Carlo integration, in which samples are drawn from the variational distribution $z ~ q(z \cond{X})$ and in whicn the derivative can be calculated. This sampling proceedures can cause issues however as it cause discontinuities in the computational graph. One approach to mitigate this is the reparameterization trick \cite{kingma2014autoencodingVB,rezende2014stochasticBackprop} in which the variational distribution  is rewritten as a function and sampling is done from some surrogate distribution. Here, we show the this approach for the Isotropic Gaussian with mean vector $\mu$ and variance $\sigma$
\begin{equation}
	f(\mu, \sigma, \epsilon) = \mu + \epsilon * \sigma, \epsilon \sim N(0, 1)
\end{equation}

With this, the expectation in the \ac{ELBO} can be rewritten with respect to the $P(\epsilon)$ and can easily be sampled from or have gradients passed through.

\begin{equation}
	E_{p{\epsilon} [ log P(X, z) - log q( f(\mu, \sigma, eps) \cond{X})]}
\end{equation}

As a means to make both the conditional distribution $P(X \cond{z})$ and $q(z \cond{X})$ computationally efficient, ammortized inference \reminder{cite something here} is employed to learn the distributions. In the regular variational inference setting, the sufficient statistics for a distribution would be learned per datum. This can be quite memory and computationally expensive, and instead these distributions are represented by functions which produce the distribution parameters. These can be any function, but typically neural networks parameterized by $\theta$ and $\phi$ are used to represent $P(X \cond{z})$ as $P_{\theta}(X \cond{z})$ and $q(z \cond{X})$ as $q_{\phi}(z \cond{X})$. This is where the name variational autoencoder comes from, as the variational distribution $q$ can be interpreted as an encoder, and conditional distribution $p$ represents the decoder as in the typical autoencoder model. 

\section{Normalizing Flows}

As previously mentioned, normalizing flows are invertible functions which transform probability distributions with a change of variables formula.

\begin{equation}
	P(z_{k}) = f_{k} \circ f_{k-1} \circ ... \circ f_{0}(p(z_{0}))
\end{equation} 

where $\circ$ is shorthand for the nested calls for functions $f_{i}$. Generally we are concerned with the log-likelihood of a distribution, and so we arrive at the following formulation


\reminder{It's blanking on me how to we come about this but we end up with the following}

\begin{equation}
	logP(z_{k}) = log(P(z_{0})) - \sum_{i=1}^{k} log |det (\frac{\delta f_{i}}{\delta z_{i-1}}) |
\end{equation}

The focus of normalizing flows research has been in finding invertible functions $f$ such that the determinant of the Jacobian are computationally efficient as that is the major bottle neck in these types of models. 

To our knowledge, there are 2 main ways normalizing flows models are optimized in the literature. The first approach is to directly include the normalizing flows into the \ac{ELBO} which introduce the log determinant as an extra term into the objective.

\begin{equation}
E_{p(\epsilon)} [ log P(X, z) - log q( f(\mu, \sigma, eps) \cond{X}) + \sum_{i=1}^{k}log |det (\frac{\delta f_{i}}{\delta z_{i-1}}) | ]
\end{equation}

In this setting, previous works typically also ammortize the parameters of the normalizing flows instead of directly optimizing them  \cite{rezende2015VIwithNF,tomczak2016Householder,vdberg2018sylvester}. 

An alternative approach, which can be more viewed as more eloquant as it does not optimize a bound like with the \ac{ELBO}, is to instead directly optimize the log probability of the data distribution.
 
\begin{equation}
	logP(X) = log(P(z_{0})) - \sum_{i=1}^{k} |det (\frac{\delta f_{i}}{\delta z_{i-1}}) |
\end{equation}

This approach is quite useful as it eliminates the need for the variational distribution as previously mentioned in the variational autoencoder. The choice of flow becomes particularly important though because some flows can be quite slow to do density estimation in order to transform the data distribution samples back to the base distribution. 




\subsection{Types of Normalizing Flows}

There are several different types of flows models. We discuss several classes of these models, but note to the reader that this does not represent an exhaustive list of existing flows models as normalizing flows are an active area of research and alternative groups may exist. 

\subsubsection{Autoregressive Models}
Autoregressive Flows models are models that structure the transformations such that a diagonal Jacobina matrix can be constructed. This formulation means the determinant can be calculated linearly as the determinant of diagonal matrices is just the product of the diagonal terms $\prod_{0}^{n} \sigma_{i}$. This is accomplished by making each dimension of the transformed distribution $z$ depend on the subsequent entries.



\begin{equation}
	 z_{i} = a + b * z_{i-1}		
\end{equation}

\subsubsection{Non-volume Presevering Flow Models} 
These are models that can be shown to have the determinant equal to one. 

\subsection{Ordinary Differential Equation Flows}

I'm not as familiar with these, but basically your flows is an ordinary differential equation. 

\subsubsection{Analytically Formulated Determinant Flow Models}
This is our own classification, but several otherwise "unclassified" flows models follow this formulation. In these types of Normalizing flows,


\reminder{Misc things I'm not quite sure where to put yet...}
As previously mentioned, \ac{LVNMT} has been an active area of research in recent years. The original discriminative model was proposed by \citet{Zhang2016VNMT} who found it to improved general translation and gave better translations on longer sequences. This model was then extended by the work of \citet{Su2018VRNMT} to sequential variational \ac{RNN}s providing again better longer sentence translations. An alternative formulation of the variational distribution was proposed by \citet{schulz2018StochasticDecoder} where the emphasis was on diversifying translations. In the generative modeling approach the idea was proposed by the works of \citet{harshil2018GNMT} which enabled word imputation translation as well as a form of unsupervised translation in the \ac{LVNMT} framework. Concurrently, \citet{eikema2018AEVNMT} found the generative formulation to be marginally better than the discriminative model without the latent variable and showed these models could be slightly more robust to domain mismatch. 

To our knowledge, the applications of normalizing flows have been considered sparsely in natural language processing, and largely focused on language modeling. \citet{bowman2015GeneratingSent} briefly mention normalizing flows in the context of variational auto-regressive language modeling, but did not report their empirical findings. \citet{ziegler2019LatentNFforDiscrete} provides empirical evidence on applying normalizing flows for character level language modeling. They proposed several non-autoregressive models that allow for parallel decoding, and provided evidence on the multi-modal behaviors of language factors. In this work, we focus on autoregressive approaches applied to neural machine translations.

Specific to research on developing normalizing flow functions, there has been recent work developing discrete normalizing flows. \cite{tran2019discreteflows,hoogeboom2019IntegerDiscreteFlows} propose similar flows that allow direct application of normalizing flows on discrete distributions. \reminder{read these papers and give the gist of them here}. \reminder{They do this by incorporating modulus mathematics and some something functions on top of this}.







