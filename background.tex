\chapter{Background}

In this section, we provide background information on variational autoencoders, normalizing flows, and give a general description of latent variable neural machine translation modeling. 

\section{Variational Autoencoders}

Variational autoencoders are a class of generative models that model the joint distribution $P(X, z)$, where $X$ is the dataset and $z$ is an introduced random variable. The joint distribution is factorized typically in the following form $P(X, z) = P(X\cond{z})P(z)$. This can be interpreted as assuming the dataset $X$ was generated by some latent process represented as a random variable $z$. To calculate the marginal probability $P(X)$, one then has to integrate out the random variable $z)$.
\begin{equation}
	P(X) = \int P(X \cond{z}) P(z) dz
\end{equation}

Unfortunately, this integral is generally considered to be intractable and a variational distribution $q(z \cond{X})$ is introduced to approximate the true $P(X)$. This is accomplished by minimizing the \ac{ELBO}, or variational free energy, the joint distribution $P(X, z)$ and $q(z \cond{X})$. 

\begin{equation}
	\E_{q(z \cond{X})} [ log(P(X, z) - log q(z \cond{X})] 
\end{equation}


In order to optimize this objective requires Monte Carlo integration, in which samples are drawn from the variational distribution $z ~ q(z \cond{X})$ and in whicn the derivative can be calculated. This sampling proceedures can cause issues however as it cause discontinuities in the computational graph. One approach to mitigate this is the reparameterization trick \cite{kingma2014autoencodingVB,rezende2014stochasticBackprop} in which the variational distribution  is rewritten as a function and sampling is done from some surrogate distribution. Here, we show the this approach for the Isotropic Gaussian with mean vector $\mu$ and variance $\sigma$
\begin{equation}
	f(\mu, \sigma, \epsilon) = \mu + \epsilon * \sigma, \epsilon \sim N(0, 1)
\end{equation}

With this, the expectation in the \ac{ELBO} can be rewritten with respect to the $P(\epsilon)$ and can easily be sampled from or have gradients passed through.

\begin{equation}
	E_{p{\epsilon} [ log P(X, z) - log q( f(\mu, \sigma, eps) \cond{X})]}
\end{equation}

As a means to make both the conditional distribution $P(X \cond{z})$ and $q(z \cond{X})$ computationally efficient, ammortized inference \reminder{cite something here} is employed to learn the distributions. In the regular variational inference setting, the sufficient statistics for a distribution would be learned per datum. This can be quite memory and computationally expensive, and instead these distributions are represented by functions which produce the distribution parameters. These can be any function, but typically neural networks parameterized by $\theta$ and $\phi$ are used to represent $P(X \cond{z})$ as $P_{\theta}(X \cond{z})$ and $q(z \cond{X})$ as $q_{\phi}(z \cond{X})$. This is where the name variational autoencoder comes from, as the variational distribution $q$ can be interpreted as an encoder, and conditional distribution $p$ represents the decoder as in the typical autoencoder model. 

\section{Normalizing Flows}

As previously mentioned, normalizing flows are invertible functions which transform probability distributions with a change of variables formula.

\begin{equation}
	P(z_{k}) = f_{k} \circ f_{k-1} \circ ... \circ f_{0}(p(z_{0}))
\end{equation} 

where $\circ$ is shorthand for the nested calls for functions $f_{i}$. Generally we are concerned with the log-likelihood of a distribution, and so we arrive at the following formulation


\reminder{It's blanking on me how to we come about this but we end up with the following}

\begin{equation}
	logP(z_{k}) = log(P(z_{0})) - \sum_{i=1}^{k} log |det (\frac{\delta f_{i}}{\delta z_{i-1}}) |
\end{equation}

The focus of normalizing flows research has been in finding invertible functions $f$ such that the determinant of the Jacobian are computationally efficient as that is the major bottle neck in these types of models. 

To our knowledge, there are 2 main ways normalizing flows models are optimized in the literature. The first approach is to directly include the normalizing flows into the \ac{ELBO} which introduce the log determinant as an extra term into the objective.

\begin{equation}
E_{p(\epsilon)} [ log P(X, z) - log q( f(\mu, \sigma, eps) \cond{X}) + \sum_{i=1}^{k}log |det (\frac{\delta f_{i}}{\delta z_{i-1}}) | ]
\end{equation}

In this setting, previous works typically also ammortize the parameters of the normalizing flows instead of directly optimizing them  \cite{rezende2015VIwithNF,tomczak2016Householder,vdberg2018sylvester}. 

An alternative approach, which can be more viewed as more eloquant as it does not optimize a bound like with the \ac{ELBO}, is to instead directly optimize the log probability of the data distribution.
 
\begin{equation}
	logP(X) = log(P(z_{0})) - \sum_{i=1}^{k} |det (\frac{\delta f_{i}}{\delta z_{i-1}}) |
\end{equation}

This approach is quite useful as it eliminates the need for the variational distribution as previously mentioned in the variational autoencoder. The choice of flow becomes particularly important though because some flows can be quite slow to do density estimation in order to transform the data distribution samples back to the base distribution. 




\subsection{Types of Normalizing Flows}

There are several different types of flows models. We discuss several classes of these models, but note to the reader that this does not represent an exhaustive list of existing flows models as normalizing flows are an active area of research and alternative groups may exist. 

\subsubsection{Autoregressive Models}
Autoregressive Flows models are models that structure the transformations such that a diagonal Jacobina matrix can be constructed. This formulation means the determinant can be calculated linearly as the determinant of diagonal matrices is just the product of the diagonal terms $\prod_{0}^{n} \sigma_{i}$. This is accomplished by making each dimension of the transformed distribution $z$ depend on the subsequent entries.



\begin{equation}
	 z_{i} = a + b * z_{i-1}		
\end{equation}

\subsubsection{Non-volume Presevering Flow Models} 
These are models that can be shown to have the determinant equal to one. 

\subsection{Ordinary Differential Equation Flows}

I'm not as familiar with these, but basically your flows is an ordinary differential equation. 

\subsubsection{Analytically Formulated Determinant Flow Models}
This is our own classification, but several otherwise "unclassified" flows models follow this formulation. In these types of Normalizing flows,


\section{Latent Variable Neural Machine Translation}

In this section, we attempt to more generally classify latent variable model neural machine translation. The two main considerations we have seen in the literature are the type of distribution to model, and the number of latent variables. 

We note that latent variables have also been considered in formulations that may not lend themselves to the framework as discussed here. An example of this is the work of \reminder{cite that paper that uses a latent variable to pick a decoder} where the latent variable is used to pick a decoder. 

\subsection{Choice of Distribution}

When we speak of the choice of distribution to represent, the two choices are either the discriminative distribution $P(Y \cond{X})$ or the joint distribution $P(Y , X)$. In either case, we introduce a latent variable 

\subsection{Number of Latent Variables}





