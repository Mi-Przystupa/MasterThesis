\chapter{Background}

In this section, we provide background information on several subjects related to the work in this thesis. This includes discussion variational auto-encoders, normalizing flows, and a brief description of \ac{NMT} where we give further details in the specific \ac{NMT} model considered in the chapter on \ac{LVNMT} models.  



\subsection{Neural Machine Translation}

In \ac{SMT}, \ac{NMT} refers specifically to training \ac{SMT} systems which incorporate neural networks as the primary model in the system. In recent years, \ac{NMT} systems have achieve \ac{SOTA} results across a variety of language pairs over alternative approaches in \ac{SMT} literature \reminder{cite, cite, cite}. In the most general framework, the objective is to maximize the log-likelihood of the observed sequences 
\begin{equation}
	max_{\theta} \log p_{\theta}(y \cond{x})  = \sum_{i=1}^{T} \log p(y_{i} \cond{x, y_{<i}})
\end{equation}\

Here, $\theta$ represents the parameters of the chosen \ac{NMT} architecture, which is optimized typically with stochastic gradient descent methods. 

There are a variety of hyperparameter choices when building an \ac{NMT} system, but one core components of many \ac{SOTA} systems include auto-regressive neural networks. In these neural networks, each step of the translation process conditions on the output of the previous output. One classification of such networks is the \ac{RNN} in which an internal hidden representation is maintained. This can be viewed as the \textit{memory} of the neural network to encode information as the sequence is predicted. In the above objective, this hidden state enables the interpretation of conditioning the curren prediction $y_{i}$ on the source input and previous words is interpreted in the distribution $p(y_{i} \cond{x, y_{<i}})$. 

As their are several types of \ac{RNN}s, we focus our discussion specifically towards \ac{GRU} which represent one type of \ac{RNN} \cite{cho2014GRU}. The network can be described with the following equations
\begin{equation}
	z_{t} = \text{sigmoid}(W_{z}[x_{t}; h_{t-1} ] ) %x_{t} + U_{z} h_{t-1} + b_{z})
\end{equation}

\begin{equation} 
	r_{t} = \text{sigmoid}(W_{r} [x_{t}; h_{t-1}] )  \circ h_{t-1}%(W_{r} + U_{r} h_{t-1} + b_{r}) 
\end{equation}

\begin{equation}
	h_{t} = (1 - z_{t}) \circ h_{t-1} + z_{t} \circ (W_{h} [x_{t}; r_{t} ]) % \text{tanh} (W_{h}x_{t} + U_{h} (r{t} \circ h_{t-1}) + b_{h}) 
\end{equation}

Here $W_{z}, W_{r}, W_{h}$ represent the learned weight matrices which include can include a bias term. For brevity $[a ; b]$ is a concatentation of the input vectors, as an equivalent view is a weight matrice per input. Intuitively, the \ac{GRU} works as a soft logic gate where the update gate $z$ controls the relevance of the previous and current state in the memory of the network. The reset gate $r$ decides the actual important of the previous information in conjunction with the new input. 

Whether choosing the \ac{GRU}, or alternatives,  all \ac{NMT} models generally follow the \ac{seq2seq} framework. In seq2seq models, the source sentences $X$ are \textit{encoded} as a series of latent representations capturing words in context information. A \textit{decoder} utilizes these hidden states, such as for initialization, to help inform the decoding process for target sentences $Y$. These hidden states are used 


The one other major consideration for \ac{NMT} is picking the best translation. A naive solution is to take the most likely word at each step of decoding. This can lead to the \textbf{garden-path problem} in which case the most likely words at the current time step lead to a series of unlikely words later in translation. Instead, typically \textit{beam search} is employed to maintain $n$ possible beams (translation hypotheses) \cite{koehn2017NMT}. At each step, after committing to the top $n$ beams  in the previous time step, a score function is calculated and the top $n$ new beams are selected. The score function is the partial probability of up to the current time step $i$, $\prod p(x_1{}$. When a beam encounters an end-of-sentence token, it is held out and the number of active beams is reduced. Once all beams are in-active, the best translation is then chosen from the remaining beams based on their probability normalized by length $p(y_{1}, ..., y_{n}) / n$. 

\reminder{You should talk about beam search / decoding choices}




\section{Variational Autoencoders}

Variational autoencoders are a class of generative models that model the joint distribution $P(X, z)$, where $X$ is the dataset and $z$ is an introduced random variable. The joint distribution is factorized typically in the following form $P(X, z) = P(X\cond{z})P(z)$. This can be interpreted as assuming the dataset $X$ was generated by some latent process represented as a random variable $z$. To calculate the marginal probability $P(X)$, one then has to integrate out the random variable $z)$.
\begin{equation}
	P(X) = \int P(X \cond{z}) P(z) dz
\end{equation}

Unfortunately, this integral is generally considered to be intractable and a variational distribution $q(z \cond{X})$ is introduced to approximate the true $P(X)$. This is accomplished by minimizing the \ac{ELBO}, or variational free energy, the joint distribution $P(X, z)$ and $q(z \cond{X})$. 

\begin{equation}
	\E_{q(z \cond{X})} [ log(P(X, z) - log q(z \cond{X})] 
\end{equation}


In order to optimize this objective requires Monte Carlo integration, in which samples are drawn from the variational distribution $z ~ q(z \cond{X})$ and in whicn the derivative can be calculated. This sampling proceedures can cause issues however as it cause discontinuities in the computational graph. One approach to mitigate this is the reparameterization trick \cite{kingma2014autoencodingVB,rezende2014stochasticBackprop} in which the variational distribution  is rewritten as a function and sampling is done from some surrogate distribution. Here, we show the this approach for the Isotropic Gaussian with mean vector $\mu$ and variance $\sigma$
\begin{equation}
	f(\mu, \sigma, \epsilon) = \mu + \epsilon * \sigma, \epsilon \sim N(0, 1)
\end{equation}

With this, the expectation in the \ac{ELBO} can be rewritten with respect to the $P(\epsilon)$ and can easily be sampled from or have gradients passed through.

\begin{equation}
	E_{p{\epsilon} [ log P(X, z) - log q( f(\mu, \sigma, eps) \cond{X})]}
\end{equation}

As a means to make both the conditional distribution $P(X \cond{z})$ and $q(z \cond{X})$ computationally efficient, ammortized inference \reminder{cite something here} is employed to learn the distributions. In the regular variational inference setting, the sufficient statistics for a distribution would be learned per datum. This can be quite memory and computationally expensive, and instead these distributions are represented by functions which produce the distribution parameters. These can be any function, but typically neural networks parameterized by $\theta$ and $\phi$ are used to represent $P(X \cond{z})$ as $P_{\theta}(X \cond{z})$ and $q(z \cond{X})$ as $q_{\phi}(z \cond{X})$. This is where the name variational autoencoder comes from, as the variational distribution $q$ can be interpreted as an encoder, and conditional distribution $p$ represents the decoder as in the typical autoencoder model. 

\section{Normalizing Flows}

As previously mentioned, normalizing flows are invertible functions which transform probability distributions with a change of variables formula.

\begin{equation}
	P(z_{k}) = f_{k} \circ f_{k-1} \circ ... \circ f_{0}(p(z_{0}))
\end{equation} 

where $\circ$ is shorthand for the nested calls for functions $f_{i}$. Generally we are concerned with the log-likelihood of a distribution, and so we arrive at the following formulation


\reminder{It's blanking on me how to we come about this but we end up with the following}

\begin{equation}
	logP(z_{k}) = log(P(z_{0})) - \sum_{i=1}^{k} log |det (\frac{\delta f_{i}}{\delta z_{i-1}}) |
\end{equation}

The focus of normalizing flows research has been in finding invertible functions $f$ such that the determinant of the Jacobian are computationally efficient as that is the major bottle neck in these types of models. 

To our knowledge, there are 2 main ways normalizing flows models are optimized in the literature. The first approach is to directly include the normalizing flows into the \ac{ELBO} which introduce the log determinant as an extra term into the objective.

\begin{equation}
E_{p(\epsilon)} [ log P(X, z) - log q( f(\mu, \sigma, eps) \cond{X}) + \sum_{i=1}^{k}log |det (\frac{\delta f_{i}}{\delta z_{i-1}}) | ]
\end{equation}

In this setting, previous works typically also ammortize the parameters of the normalizing flows instead of directly optimizing them  \cite{rezende2015VIwithNF,tomczak2016Householder,vdberg2018sylvester}. 

An alternative approach, which can be more viewed as more eloquant as it does not optimize a bound like with the \ac{ELBO}, is to instead directly optimize the log probability of the data distribution.
 
\begin{equation}
	logP(X) = log(P(z_{0})) - \sum_{i=1}^{k} |det (\frac{\delta f_{i}}{\delta z_{i-1}}) |
\end{equation}

This approach is quite useful as it eliminates the need for the variational distribution as previously mentioned in the variational autoencoder. The choice of flow becomes particularly important though because some flows can be quite slow to do density estimation in order to transform the data distribution samples back to the base distribution. 




\subsection{Types of Normalizing Flows}

There are several different types of flows models. We discuss several classes of these models, but note to the reader that this does not represent an exhaustive list of existing flows models as normalizing flows are an active area of research and alternative groups may exist. 

\subsubsection{Autoregressive Models}
Autoregressive Flows models are models that structure the transformations such that a diagonal Jacobina matrix can be constructed. This formulation means the determinant can be calculated linearly as the determinant of diagonal matrices is just the product of the diagonal terms $\prod_{0}^{n} \sigma_{i}$. This is accomplished by making each dimension of the transformed distribution $z$ depend on the subsequent entries.



\begin{equation}
	 z_{i} = a + b * z_{i-1}		
\end{equation}

\subsubsection{Non-volume Presevering Flow Models} 
These are models that can be shown to have the determinant equal to one. 

\subsection{Ordinary Differential Equation Flows}

I'm not as familiar with these, but basically your flows is an ordinary differential equation. 

\subsubsection{Analytically Formulated Determinant Flow Models}
This is our own classification, but several otherwise "unclassified" flows models follow this formulation. In these types of Normalizing flows,


\reminder{Misc things I'm not quite sure where to put yet...}
As previously mentioned, \ac{LVNMT} has been an active area of research in recent years. The original discriminative model was proposed by \citet{Zhang2016VNMT} who found it to improved general translation and gave better translations on longer sequences. This model was then extended by the work of \citet{Su2018VRNMT} to sequential variational \ac{RNN}s providing again better longer sentence translations. An alternative formulation of the variational distribution was proposed by \citet{schulz2018StochasticDecoder} where the emphasis was on diversifying translations. In the generative modeling approach the idea was proposed by the works of \citet{harshil2018GNMT} which enabled word imputation translation as well as a form of unsupervised translation in the \ac{LVNMT} framework. Concurrently, \citet{eikema2018AEVNMT} found the generative formulation to be marginally better than the discriminative model without the latent variable and showed these models could be slightly more robust to domain mismatch. 

To our knowledge, the applications of normalizing flows have been considered sparsely in natural language processing, and largely focused on language modeling. \citet{bowman2015GeneratingSent} briefly mention normalizing flows in the context of variational auto-regressive language modeling, but did not report their empirical findings. \citet{ziegler2019LatentNFforDiscrete} provides empirical evidence on applying normalizing flows for character level language modeling. They proposed several non-autoregressive models that allow for parallel decoding, and provided evidence on the multi-modal behaviors of language factors. In this work, we focus on autoregressive approaches applied to neural machine translations.

Specific to research on developing normalizing flow functions, there has been recent work developing discrete normalizing flows. \cite{tran2019discreteflows,hoogeboom2019IntegerDiscreteFlows} propose similar flows that allow direct application of normalizing flows on discrete distributions. \reminder{read these papers and give the gist of them here}. \reminder{They do this by incorporating modulus mathematics and some something functions on top of this}.







