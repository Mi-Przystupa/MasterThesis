@incollection{sutskever2014seq2seq,
	title = {Sequence to Sequence Learning with Neural Networks},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	booktitle = {Advances in Neural Information Processing Systems 27},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	pages = {3104--3112},
	year = {2014},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}

@inproceedings{vinyals2015grammaras,
	author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
	title = {Grammar as a Foreign Language},
	year = {2015},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
	pages = {2773–2781},
	numpages = {9},
	location = {Montreal, Canada},
	series = {NIPS’15}
}

@inproceedings{lafferty2001CRF,
	author = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
	title = {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
	year = {2001},
	isbn = {1558607781},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
	pages = {282–289},
	numpages = {8},
	series = {ICML ’01}
}

@article{sphericallatent2018Xu,
	author =      "Xu, Jiacheng and Durrett, Greg",
	title =       "Spherical Latent Spaces for Stable Variational Autoencoders",
	booktitle =   "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	year =        "2018",
}

@unknown{flowseq2019Xuezhe,
	author = {Ma, Xuezhe and Zhou, Chunting and Li, Xian and Neubig, Graham and Hovy, Eduard},
	year = {2019},
	month = {09},
	pages = {},
	title = {FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow}
}

@InProceedings{MADE2015germain,
	title = 	 {MADE: Masked Autoencoder for Distribution Estimation},
	author = 	 {Mathieu Germain and Karol Gregor and Iain Murray and Hugo Larochelle},
	booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
	pages = 	 {881--889},
	year = 	 {2015},
	editor = 	 {Francis Bach and David Blei},
	volume = 	 {37},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Lille, France},
	month = 	 {07--09 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
	url = 	 {http://proceedings.mlr.press/v37/germain15.html},
	abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}

@InProceedings{cohen2019unconstrained,
	title = 	 {Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models},
	author = 	 {Cohen, Eldan and Beck, Christopher},
	booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
	pages = 	 {1290--1299},
	year = 	 {2019},
	editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	volume = 	 {97},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Long Beach, California, USA},
	month = 	 {09--15 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v97/cohen19a/cohen19a.pdf},
	url = 	 {http://proceedings.mlr.press/v97/cohen19a.html},
	abstract = 	 {Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, work on a number of applications has found that the quality of the highest probability hypothesis found by beam search degrades with large beam widths. We perform an empirical study of the behavior of beam search across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early, very low probability tokens that are followed by a sequence of tokens with higher (conditional) probability. We show that, empirically, such sequences are more likely to have a lower evaluation score than lower probability sequences without this pattern. Using the notion of search discrepancies from heuristic search, we hypothesize that large discrepancies are the cause of the performance degradation. We show that this hypothesis generalizes the previous ones in machine translation and image captioning. To validate our hypothesis, we show that constraining beam search to avoid large discrepancies eliminates the performance degradation.}
}

@InProceedings{gregor2015MADE,
	title = 	 {MADE: Masked Autoencoder for Distribution Estimation},
	author = 	 {Mathieu Germain and Karol Gregor and Iain Murray and Hugo Larochelle},
	booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
	pages = 	 {881--889},
	year = 	 {2015},
	editor = 	 {Francis Bach and David Blei},
	volume = 	 {37},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Lille, France},
	month = 	 {07--09 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
	url = 	 {http://proceedings.mlr.press/v37/germain15.html},
	abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}

@article{tabak2013familyofnonparametricdensity,
	author = {Tabak, E. G. and Turner, Cristina V.},
	title = {A Family of Nonparametric Density Estimation Algorithms},
	journal = {Communications on Pure and Applied Mathematics},
	volume = {66},
	number = {2},
	pages = {145-164},
	doi = {10.1002/cpa.21423},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21423},
	abstract = {Abstract A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. © 2012 Wiley Periodicals, Inc.},
	year = {2013}
}

@article{tabak2010densityestimationdual,
	title = "Density estimation by dual ascent of the log-likelihood",
	abstract = "A methodology is developed to assign, from an observed sample, a joint-probability distribution to a set of continuous variables. The algorithm proposed performs this assignment by mapping the original variables onto a jointly-Gaussian set. The map is built iteratively, ascending the log-likelihood of the observations, through a series of steps that move the marginal distributions along a random set of orthogonal directions towards normality.",
	keywords = "Density estimation, Machine learning, Maximum likelihood",
	author = "Esteban Tabak and {Vanden Eijnden}, Eric",
	year = "2010",
	language = "English (US)",
	volume = "8",
	pages = "217--233",
	journal = "Communications in Mathematical Sciences",
	issn = "1539-6746",
	publisher = "International Press of Boston, Inc.",
	number = "1",
}

@incollection{papamakarios2017MAF,
	title = {Masked Autoregressive Flow for Density Estimation},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	booktitle = {Advances in Neural Information Processing Systems 30},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {2338--2347},
	year = {2017},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6828-masked-autoregressive-flow-for-density-estimation.pdf}
}

@inproceedings{ha2016hypernets,
	title	= {HyperNetworks},
	author	= {David Ha and Andrew Dai and Quoc Le},
	year	= {2016}
}



@article{vaswani2017attentionTransformer,
	title = {Attention is All you Need},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems 30},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {5998--6008},
	year = {2017},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@article{koehn2017NMT,
	author    = {Philipp Koehn},
	title     = {Neural Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1709.07809},
	year      = {2017}
}

@inproceedings{he2018seq2seqmixturemodelforDiverseMachineTranslation,
    title = "Sequence to Sequence Mixture Model for Diverse Machine Translation",
    author = "He, Xuanli  and
    Haffari, Gholamreza  and
    Norouzi, Mohammad",
    booktitle = "Proceedings of the 22nd Conference on Computational Natural Language Learning",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K18-1056",
    doi = "10.18653/v1/K18-1056",
    pages = "583--592",
    abstract = "Sequence to sequence (SEQ2SEQ) models lack diversity in their generated translations. This can be attributed to their limitations in capturing lexical and syntactic variations in parallel corpora, due to different styles, genres, topics, or ambiguity of human translation process. In this paper, we develop a novel sequence to sequence mixture (S2SMIX) model that improves both translation diversity and quality by adopting a committee of specialized translation models rather than a single translation model. Each mixture component selects its own training dataset via optimization of the marginal log-likelihood, which leads to a soft clustering of the parallel corpus. Experiments on four language pairs demonstrate the superiority of our mixture model compared to SEQ2SEQ model with the standard and diversity encouraged beam search. Our mixture model incurs negligible additional parameters and no extra computation in the decoding time.",
}

@InProceedings{kaiser2018DiscreteLatentVariables,
	title = 	 {Fast Decoding in Sequence Models Using Discrete Latent Variables},
	author = 	 {Kaiser, Lukasz and Bengio, Samy and Roy, Aurko and Vaswani, Ashish and Parmar, Niki and Uszkoreit, Jakob and Shazeer, Noam},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {2390--2399},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf},
	url = 	 {http://proceedings.mlr.press/v80/kaiser18a.html},
	abstract = 	 {Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and Transformer are the state-of-the-art on many tasks. However, they lack parallelism and are thus slow for long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallel during training, but still lack parallelism during decoding. We present a method to extend sequence models using discrete latent variables that makes decoding much more parallel. The main idea behind this approach is to first autoencode the target sequence into a shorter discrete latent sequence, which is generated autoregressively, and finally decode the full sequence from this shorter latent sequence in a parallel manner. To this end, we introduce a new method for constructing discrete latent variables and compare it with previously introduced methods. Finally, we verify that our model works on the task of neural machine translation, where our models are an order of magnitude faster than comparable autoregressive models and, while lower in BLEU than purely autoregressive models, better than previously proposed non-autogregressive translation.}
}

@inproceedings{shen2019mixturemodelsfordiverseMT,
  author    = {Tianxiao Shen and
  Myle Ott and
  Michael Auli and
  Marc'Aurelio Ranzato},
  editor    = {Kamalika Chaudhuri and
  Ruslan Salakhutdinov},
  title     = {Mixture Models for Diverse Machine Translation: Tricks of the Trade},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {5719--5728},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/shen19c.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/ShenOAR19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{tran2019discreteflows,
  author    = {Dustin Tran and
  Keyon Vafa and
  Kumar Krishna Agrawal and
  Laurent Dinh and
  Ben Poole},
  editor    = {Hanna M. Wallach and
  Hugo Larochelle and
  Alina Beygelzimer and
  Florence d'Alch{\'{e}}{-}Buc and
  Emily B. Fox and
  Roman Garnett},
  title     = {Discrete Flows: Invertible Generative Models of Discrete Data},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
  on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
  December 2019, Vancouver, BC, Canada},
  pages     = {14692--14701},
  year      = {2019},
  url       = {http://papers.nips.cc/paper/9612-discrete-flows-invertible-generative-models-of-discrete-data},
  timestamp = {Mon, 13 Jan 2020 09:28:31 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/TranVADP19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hoogeboom2019IntegerDiscreteFlows,
  author    = {Emiel Hoogeboom and
  Jorn W. T. Peters and
  Rianne van den Berg and
  Max Welling},
  editor    = {Hanna M. Wallach and
  Hugo Larochelle and
  Alina Beygelzimer and
  Florence d'Alch{\'{e}}{-}Buc and
  Emily B. Fox and
  Roman Garnett},
  title     = {Integer Discrete Flows and Lossless Compression},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
  on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14
  December 2019, Vancouver, BC, Canada},
  pages     = {12134--12144},
  year      = {2019},
  url       = {http://papers.nips.cc/paper/9383-integer-discrete-flows-and-lossless-compression},
  timestamp = {Mon, 13 Jan 2020 09:28:31 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/HoogeboomPBW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zhang2016VNMT,
    title = "Variational Neural Machine Translation",
    author = "Zhang, Biao  and
    Xiong, Deyi  and
    Su, Jinsong  and
    Duan, Hong  and
    Zhang, Min",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1050",
    doi = "10.18653/v1/D16-1050",
    pages = "521--530",
}

@misc{
	shen2019diverse,
	title={Diverse Machine Translation with a Single Multinomial Latent Variable},
	author={Tianxiao Shen and Myle Ott and Michael Auli and Marc’Aurelio Ranzato},
	year={2019},
	url={https://openreview.net/forum?id=BJgnmhA5KQ},
}

@incollection{harshil2018GNMT,
	title = {Generative Neural Machine Translation},
	author = {Shah, Harshil and Barber, David},
	booktitle = {Advances in Neural Information Processing Systems 31},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {1346--1355},
	year = {2018},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/7409-generative-neural-machine-translation.pdf}
}

@inproceedings{Su2018VRNMT,
  added-at = {2019-11-18T00:00:00.000+0100},
  author = {Su, Jinsong and Wu, Shan and Xiong, Deyi and Lu, Yaojie and Han, Xianpei and Zhang, Biao},
  biburl = {https://www.bibsonomy.org/bibtex/208bff003dcae1031b2daa6cc35bd2bc5/dblp},
  booktitle = {AAAI},
  crossref = {conf/aaai/2018},
  editor = {McIlraith, Sheila A. and Weinberger, Kilian Q.},
  ee = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16791},
  interhash = {a25dd3bdabbf7b21107afa991f9856a2},
  intrahash = {08bff003dcae1031b2daa6cc35bd2bc5},
  keywords = {dblp},
  pages = {5488-5495},
  publisher = {AAAI Press},
  timestamp = {2019-11-19T11:46:32.000+0100},
  title = {Variational Recurrent Neural Machine Translation.},
  url = {http://dblp.uni-trier.de/db/conf/aaai/aaai2018.html\#SuWXLHZ18},
  year = 2018
}


@inproceedings{schulz2018StochasticDecoder,
	title = "A Stochastic Decoder for Neural Machine Translation",
	author = "Schulz, Philip  and
	Aziz, Wilker  and
	Cohn, Trevor",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P18-1115",
	pages = "1243--1252",
	abstract = "The process of translation is ambiguous, in that there are typically many valid translations for a given sentence. This gives rise to significant variation in parallel corpora, however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process. To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora. We provide an in-depth analysis of the pitfalls encountered in variational inference for training deep generative models. Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.",
}

@inproceedings{sennrich2015ImprovingNMT,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
    Haddow, Barry  and
    Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}

@inproceedings{edunov2018understandigBackTrans,
    title = "Understanding Back-Translation at Scale",
    author = "Edunov, Sergey  and
    Ott, Myle  and
    Auli, Michael  and
    Grangier, David",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1045",
    doi = "10.18653/v1/D18-1045",
    pages = "489--500",
    abstract = "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT{'}14 English-German test set.",
}

@inproceedings{eikema2018AEVNMT,
    title = "Auto-Encoding Variational Neural Machine Translation",
    author = "Eikema, Bryan  and
    Aziz, Wilker",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4315",
    doi = "10.18653/v1/W19-4315",
    pages = "124--141",
    abstract = "We present a deep generative model of bilingual sentence pairs for machine translation. The model generates source and target sentences jointly from a shared latent representation and is parameterised by neural networks. We perform efficient training using amortised variational inference and reparameterised gradients. Additionally, we discuss the statistical implications of joint modelling and propose an efficient approximation to maximum a posteriori decoding for fast test-time predictions. We demonstrate the effectiveness of our model in three machine translation scenarios: in-domain training, mixed-domain training, and learning from a mix of gold-standard and synthetic data. Our experiments show consistently that our joint formulation outperforms conditional modelling (i.e. standard neural machine translation) in all such scenarios.",
}

@InProceedings{rezende2014stochasticBackprop,
	title = 	 {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
	author = 	 {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
	booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
	pages = 	 {1278--1286},
	year = 	 {2014},
	editor = 	 {Eric P. Xing and Tony Jebara},
	volume = 	 {32},
	number =       {2},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bejing, China},
	month = 	 {22--24 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v32/rezende14.pdf},
	url = 	 {http://proceedings.mlr.press/v32/rezende14.html},
	abstract = 	 {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning.   Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound.  We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and   derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models.  We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to  generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}

@inproceedings{kingma2014autoencodingVB,
	author    = {Diederik P. Kingma and
	Max Welling},
	title     = {Auto-Encoding Variational Bayes},
	booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
	Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
	year      = {2014},
	crossref  = {DBLP:conf/iclr/2014},
	url       = {http://arxiv.org/abs/1312.6114},
	timestamp = {Fri, 29 Mar 2019 10:36:36 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaW13},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{rezende2015VIwithNF,
	title = 	 {Variational Inference with Normalizing Flows},
	author = 	 {Danilo Rezende and Shakir Mohamed},
	booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
	pages = 	 {1530--1538},
	year = 	 {2015},
	editor = 	 {Francis Bach and David Blei},
	volume = 	 {37},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Lille, France},
	month = 	 {07--09 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
	url = 	 {http://proceedings.mlr.press/v37/rezende15.html},
	abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}

@inproceedings{bowman2015GeneratingSent,
    title = "Generating Sentences from a Continuous Space",
    author = "Bowman, Samuel R.  and
    Vilnis, Luke  and
    Vinyals, Oriol  and
    Dai, Andrew  and
    Jozefowicz, Rafal  and
    Bengio, Samy",
    booktitle = "Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K16-1002",
    doi = "10.18653/v1/K16-1002",
    pages = "10--21",
}

@inproceedings{bahdanau2014NMTBYJoint,
  author    = {Dzmitry Bahdanau and
  Kyunghyun Cho and
  Yoshua Bengio},
  editor    = {Yoshua Bengio and
  Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{bastings2018annotated,
	title={The Annotated Encoder-Decoder with Attention},
	author={Bastings, Joost},
	journal={https://bastings.github.io/annotated\_encoder\_decoder/},
	year={2018}
}

@inproceedings{zbib2012MTArabicDial,
	author = {Zbib, Rabih and Malchiodi, Erika and Devlin, Jacob and Stallard, David and Matsoukas, Spyros and Schwartz, Richard and Makhoul, John and Zaidan, Omar F. and Callison-Burch, Chris},
	title = {Machine Translation of Arabic Dialects},
	booktitle = {Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	series = {NAACL HLT '12},
	year = {2012},
	isbn = {978-1-937284-20-6},
	location = {Montreal, Canada},
	pages = {49--59},
	numpages = {11},
	url = {http://dl.acm.org/citation.cfm?id=2382029.2382037},
	acmid = {2382037},
	publisher = {Association for Computational Linguistics},
	address = {Stroudsburg, PA, USA},
} 


@InProceedings{ziegler2019LatentNFforDiscrete,
  title = 	 {Latent Normalizing Flows for Discrete Sequences},
  author = 	 {Ziegler, Zachary and Rush, Alexander},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7673--7682},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ziegler19a/ziegler19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/ziegler19a.html},
  abstract = 	 {Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.}
}

@book{koehnSMT2010,
	author = {Koehn, Philipp},
	title = {Statistical Machine Translation},
	year = {2010},
	isbn = {0521874157, 9780521874151},
	edition = {1st},
	publisher = {Cambridge University Press},
	address = {New York, NY, USA},
} 

@inproceedings{lample2018PhraseNeuralUnsuperviseMT,
  title={Phrase-Based \& Neural Unsupervised Machine Translation},
  author={Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2018}
}

@inproceedings{Lample2017UnsuperviseMT,
  author    = {Guillaume Lample and
  Alexis Conneau and
  Ludovic Denoyer and
  Marc'Aurelio Ranzato},
  title     = {Unsupervised Machine Translation Using Monolingual Corpora Only},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=rkYTTf-AZ},
  timestamp = {Thu, 25 Jul 2019 14:25:53 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LampleCDR18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{artetxe2017UnsupervisedNMT,
    title = "Unsupervised Statistical Machine Translation",
    author = "Artetxe, Mikel  and
    Labaka, Gorka  and
    Agirre, Eneko",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1399",
    doi = "10.18653/v1/D18-1399",
    pages = "3632--3642",
    abstract = "While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at \url{https://github.com/artetxem/monoses}.",
}

@incollection{sonderby2016LadderVAE,
	title = {Ladder Variational Autoencoders},
	author = {S\o nderby, Casper Kaae and Raiko, Tapani and Maal\o e, Lars and S\o nderby, S\o ren Kaae and Winther, Ole},
	booktitle = {Advances in Neural Information Processing Systems 29},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	pages = {3738--3746},
	year = {2016},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6275-ladder-variational-autoencoders.pdf}
}

@inproceedings{cho2014GRU,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
    van Merri{\"e}nboer, Bart  and
    Gulcehre, Caglar  and
    Bahdanau, Dzmitry  and
    Bougares, Fethi  and
    Schwenk, Holger  and
    Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{hochreiter1997LSTM,
	author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
	title = {Long Short-Term Memory},
	journal = {Neural Comput.},
	issue_date = {November 15, 1997},
	volume = {9},
	number = {8},
	month = nov,
	year = {1997},
	issn = {0899-7667},
	pages = {1735--1780},
	numpages = {46},
	url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	acmid = {1246450},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 

@article{zhao2017InfoVAE,
  author    = {Shengjia Zhao and
  Jiaming Song and
  Stefano Ermon},
  title     = {InfoVAE: Balancing Learning and Inference in Variational Autoencoders},
  booktitle = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI}
  2019, The Thirty-First Innovative Applications of Artificial Intelligence
  Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational
  Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii,
  USA, January 27 - February 1, 2019},
  pages     = {5885--5892},
  publisher = {{AAAI} Press},
  year      = {2019},
  url       = {https://doi.org/10.1609/aaai.v33i01.33015885},
  doi       = {10.1609/aaai.v33i01.33015885},
  timestamp = {Wed, 25 Sep 2019 11:05:09 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/ZhaoSE19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Burgess2018UnderstandingdisentBVAE,
	author    = {Christopher P. Burgess and
	Irina Higgins and
	Arka Pal and
	Lo{\"{\i}}c Matthey and
	Nick Watters and
	Guillaume Desjardins and
	Alexander Lerchner},
	title     = {Understanding disentangling in {\(\beta\)}-VAE},
	journal   = {CoRR},
	volume    = {abs/1804.03599},
	year      = {2018},
	url       = {http://arxiv.org/abs/1804.03599},
	archivePrefix = {arXiv},
	eprint    = {1804.03599},
	timestamp = {Mon, 13 Aug 2018 01:00:00 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-03599},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bingham2018pyro,
	author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and
	Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and
	Horsfall, Paul and Goodman, Noah D.},
	title = {{Pyro: Deep Universal Probabilistic Programming}},
	journal = {Journal of Machine Learning Research},
	year = {2018}
}

@inproceedings{sennrich2015NMTRarwordsBPE,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
    Haddow, Barry  and
    Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@article{janwillem2018IntrotoProbProg,
	author    = {Jan{-}Willem van de Meent and
	Brooks Paige and
	Hongseok Yang and
	Frank Wood},
	title     = {An Introduction to Probabilistic Programming},
	journal   = {CoRR},
	volume    = {abs/1809.10756},
	year      = {2018},
	url       = {http://arxiv.org/abs/1809.10756},
	archivePrefix = {arXiv},
	eprint    = {1809.10756},
	timestamp = {Fri, 05 Oct 2018 01:00:00 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1809-10756},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{koehn2017NMT,
	author    = {Philipp Koehn},
	title     = {Neural Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1709.07809},
	year      = {2017}
}

@inproceedings{chen2016VariationalLossyAE,
  author    = {Xi Chen and
  Diederik P. Kingma and
  Tim Salimans and
  Yan Duan and
  Prafulla Dhariwal and
  John Schulman and
  Ilya Sutskever and
  Pieter Abbeel},
  title     = {Variational Lossy Autoencoder},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  crossref  = {DBLP:conf/iclr/2017},
  url       = {https://openreview.net/forum?id=BysvGP5ee},
  timestamp = {Thu, 25 Jul 2019 14:25:55 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/0022KSDDSSA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{kingma2016IAF,
	author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	title = {Improved Variational Inference with Inverse Autoregressive Flow},
	year = {2016},
	isbn = {9781510838819},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	pages = {4743–4751},
	numpages = {9},
	location = {Barcelona, Spain},
	series = {NIPS’16}
}

@inproceedings{Papineni2002BLEU,
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	title = {BLEU: A Method for Automatic Evaluation of Machine Translation},
	booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
	series = {ACL '02},
	year = {2002},
	location = {Philadelphia, Pennsylvania},
	pages = {311--318},
	numpages = {8},
	url = {https://doi.org/10.3115/1073083.1073135},
	doi = {10.3115/1073083.1073135},
	acmid = {1073135},
	publisher = {Association for Computational Linguistics},
	address = {Stroudsburg, PA, USA},
} 

@article{tomczak2016Householder,
  author    = {Jakub M. Tomczak and
  Max Welling},
  title     = {Improving Variational Auto-Encoders using Householder Flow},
  journal   = {CoRR},
  volume    = {abs/1611.09630},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09630},
  archivePrefix = {arXiv},
  eprint    = {1611.09630},
  timestamp = {Mon, 13 Aug 2018 16:46:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/TomczakW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{kingma2018GLOW,
	title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
	author = {Kingma, Durk P and Dhariwal, Prafulla},
	booktitle = {Advances in Neural Information Processing Systems 31},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	pages = {10215--10224},
	year = {2018},
	publisher = {Curran Associates, Inc.}
}


@inproceedings{Berg2018SylvesterNF,
	title={Sylvester Normalizing Flows for Variational Inference},
	author={Rianne van den Berg and Leonard Hasenclever and Jakub M. Tomczak and Max Welling},
	booktitle={UAI},
	year={2018}
}

@InProceedings{post2018SacreBLEU,
	author = 	"Post, Matt",
	title = 	"A Call for Clarity in Reporting {BLEU} Scores",
	booktitle = 	"Proceedings of the Third Conference on Machine Translation: Research Papers",
	year = 	"2018",
	publisher = 	"Association for Computational Linguistics",
	pages = 	"186--191",
	location = 	"Belgium, Brussels",
	url = 	"http://aclweb.org/anthology/W18-6319"
}

@inproceedings{
	he2018lagging,
	title={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders},
	author={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=rylDfnCqF7},
}

@inproceedings{vdberg2018sylvester,
	title={Sylvester normalizing flows for variational inference},
	author={van den Berg, Rianne and Hasenclever, Leonard and Tomczak, Jakub and Welling, Max},
	booktitle={proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)},
	year={2018}
}

@InProceedings{goodfellow2013maxout,
	title = 	 {Maxout Networks},
	author = 	 {Ian Goodfellow and David Warde-Farley and Mehdi Mirza and Aaron Courville and Yoshua Bengio},
	booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
	pages = 	 {1319--1327},
	year = 	 {2013},
	editor = 	 {Sanjoy Dasgupta and David McAllester},
	volume = 	 {28},
	number =       {3},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Atlanta, Georgia, USA},
	month = 	 {17--19 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v28/goodfellow13.pdf},
	url = 	 {http://proceedings.mlr.press/v28/goodfellow13.html},
	abstract = 	 {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.}
}


@book{gravessupervisedSequenceRNNbook2012,
	added-at = {2016-12-07T19:03:54.000+0100},
	author = {Graves, Alex},
	biburl = {https://www.bibsonomy.org/bibtex/22fe5732cd8b62f4d09a140d5b40c82ec/hprop},
	interhash = {ce5e3e2888eb4afd21867cfb9639bc23},
	intrahash = {2fe5732cd8b62f4d09a140d5b40c82ec},
	keywords = {books machine-learning rnn},
	timestamp = {2016-12-07T19:03:54.000+0100},
	title = {Supervised Sequence Labelling with Recurrent Neural Networks},
	year = 2011
}

